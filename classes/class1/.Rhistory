knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "C:\Uni\2. Semester\Methods 02\methods02")
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "C:\\Uni\\2. Semester\\Methods 02\\methods02")
getwd()
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "\\Uni\\2. Semester\\Methods 02\\methods02")
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Uni/2. Semester/Methods 02/methods02")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Uni/2. Semester/Methods 02/methods02")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Uni/2. Semester/Methods 02/methods02")
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "C:/Uni/2. Semester/Methods 02/wmethods02")
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "C:/Uni/2. Semester/Methods 02/methods02")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "C:/Uni/2. Semester/Methods 02/methods02")
getwd()
# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "C:/Uni/2. Semester/Methods 02/methods02/classes")
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
install.packages("rstanarm")
library(rstanarm)
# Load the rest
library(pacman)
pacman::p_load(tidyverse,
ggpubr,
ggplot2,
stringr) # this time I'm just giving you the code
# Load data
hibbs <- read.table("data/ElectionsEconomy/data/hibbs.dat", header = TRUE)
View(hibbs)
getwd()
# Load data
hibbs <- read.table("data/ElectionsEconomy/data/hibbs.dat", header = TRUE)
# Make scatterplot
plot(hibbs$growth, hibbs$vote, xlab="Average recent growth in personal income",
ylab="Incumbent party's vote share")
# Estimate regression y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)
# Add a fitted line to the graph
abline(coef(M1), col="gray") # needs to be run with the plot() code above - running the whole chunk is the easiest way
# Display the fitted model
print(M1)
# Basic plot with ggplot2
ggplot(hibbs, aes(x = growth, y = vote)) +
geom_point() +  # Add points
labs(
x = "Average recent growth in personal income",
y = "Incumbent party's vote share",
title = "Relationship between Income Growth and Vote Share",
subtitle = "Data from Hibbs Dataset"
) +
theme_minimal() +  # Use a minimal theme
theme(
plot.title = element_text(hjust = 0.5),  # Center the title
plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
) +
geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line
# your code here
sim_data <- rnorm(5000, 300, 30)
set.seed(1998) # setting a seed (in the best year ever??) - this way, even though it's random, you'll get reproducible results next time you run this with this seed
# rnorm() works like: my_simulated_data <- rnorm(n, mean, sd) - now you go!
# your code here
sim_data <- rnorm(5000, 300, 30)
?ggplot()
view(sim_data)
sim_data %>% ggplot() +
gghist()
sim_data <- as.tibble(sim_data)
sim_data <- as_tibble(sim_data)
sim_data <- as_tibble(sim_data)
sim_data %>% ggplot() +
gghist()
sim_data %>% ggplot() +
geom_hist()
sim_data %>% ggplot() +
geom_histogram()
sim_data %>% ggplot() +
geom_histogram(fill = count)
sim_data %>% ggplot(aes(x = x)) +
geom_histogram(fill = count)
View(sim_data)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/Users/runet/OneDrive/Skrivebord/CogCom')
# import/load packages
# ggrepel is a package that allow us to show text in ggplot without it overlapping with the data points (it provides an improved alternative to geom_text())
# ape is a package that we can use to make fancy dendogram network plots
pacman::p_load(tidyverse, ggrepel, ape, plyr, dplyr, ggplot2, lme4, lmerTest, Matrix, data.table, afex)
# A semantic distance function (that you will use later)
distance <- function(word1, word2, tvectors) {
rows <- tryCatch({
tvectors[c(word1, word2),] %>% nrow
},
error=function(cond) {
NA
})
if(is.na(rows) | rows != 2) return(NA)
vec <- tvectors[c(word1, word2), ]
diff <- c()
for(col in colnames(vec)) {
diff <- c(diff,
sqrt((vec[word1, col] - vec[word2, col])^2))
}
diff %>%
sum
}
### Normality check function
normalitycheck <- function(variable){
column_name <- deparse(substitute(variable))
print(column_name)
print(pastecs::stat.desc(variable, norm = TRUE, basic = FALSE))
print(shapiro.test(log(variable)))
print(shapiro.test(sqrt(variable)))
print(shapiro.test(1 / variable))
}
# Load the semantic model which is trained on 2 Billion word corpus, which consist of the British National Corpus (BNC), the ukWaC corpus and a 2009 Wikipedia dump
load("EN_100k.rda")
# Here you can play with the function that will return the semantic distance between two words
distance("lion", "tiger", tvectors=EN_100k)
# Loading logfile
df <- read_csv("participant logfiles/logfile_961bb98a-5376-4cb1-bd4b-e890758dfd12.csv")
df <- df %>%
select(global_time, id, word, reaction_time)
# Read all files
temp = list.files(path = "logfiles cleaned", full.names = T, pattern="*.csv")
# Load them all in
data <- ldply(temp, read_csv)
#this replaces all "don't wish to answer" with "no" in the quality column
data[data$quality == "don't wish to answer", "quality"] <- "no"
#then i select the necessary columns
data <- data %>%
select(time_of_day, global_time, id, word, reaction_time, condition, quality, group, gender) %>%
filter(condition == "verbal")
#making columns factors
data <- data %>%
mutate(quality = factor(quality)) %>%
mutate(condition = factor(condition)) %>%
mutate(gender = factor(gender)) %>%
mutate(group = factor(group))
#Assigning trial number
data <- data %>%
setDT(data)
data[, trial_number := rowid(id)]
#making back into a tibble
data <- as_tibble(data)
#Adding a column describing semantic distance between words
data <- data %>%
filter(word %in% rownames(EN_100k)) %>%
mutate(word2 = lag(word)) %>%
rowwise() %>%
dplyr::mutate(sem_dist = distance(word, word2, tvectors=EN_100k)) %>%
mutate(sem_dist = scale(sem_dist, center = FALSE)) %>%
drop_na()
# Investigating word tally
data %>%
group_by(word) %>%
tally %>%
arrange(desc(n)) %>%
mutate(word = fct_reorder(word, n)) %>%
head(15) %>%
ggplot() +
aes(n, word, fill = n) +
geom_col() +
theme_minimal() +
theme(legend.position = "none")
data %>%
dplyr::mutate(
word2 = lag(word)
) %>%
rowwise() %>%
dplyr::mutate(
sem_dist = distance(word, word2, EN_100k)
)
distance("thanksgiving", "turkey", EN_100k)
# Add a column with the semantic distance between each row of the animal words
df <- df %>%
filter(word %in% rownames(EN_100k)) %>%
mutate(word2 = lag(word)) %>%
rowwise() %>%
dplyr::mutate(sem_dist = distance(word, word2, tvectors=EN_100k)) %>%
mutate(sem_dist = scale(sem_dist, center = FALSE)) %>%
drop_na()
df %>%
ggplot(aes(global_time, reaction_time, label=word, color = reaction_time)) +
geom_line(color = "red") +
geom_text_repel()
df %>%
ggplot(aes(global_time, sem_dist, label=word,  color = sem_dist)) +
geom_line(color = "red") +
geom_text_repel()
df %>%
ggplot(aes(sem_dist, reaction_time, label = word, color = reaction_time)) +
geom_point() +
geom_smooth(method = lm, color = "red") +
geom_text_repel() +
labs(x = "semantic distance", y = "reaction time")
model <- lmerTest::lmer(reaction_time ~ sem_dist + (1|id) + (1|trial_number), data = data, REML = F)
plot(model)
summary(model)
df <- read_csv("logfiles cleaned/logfile_1673c2cc-e54a-4f86-a077-821474da47ff.csv")
df <- df %>%
select(global_time, id, word, reaction_time)
# Add a column with the semantic distance between each row of the animal words
df <- df %>%
filter(word %in% rownames(EN_100k)) %>%
mutate(word2 = lag(word)) %>%
rowwise() %>%
dplyr::mutate(sem_dist = distance(word, word2, tvectors=EN_100k)) %>%
mutate(sem_dist = scale(sem_dist, center = FALSE)) %>%
drop_na()
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(df),nrow=nrow(df)))
colnames(df2) <- df$word
rownames(df2) <- df$word
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(df$sem_dist)){
for (c in 1:length(df$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r], rownames(df2)[c], tvectors=EN_100k)
}
}
# now we use some unsupervised clustering on the semantic distances to explore how the animals cluster together as a function of their semantic distance
dd <- dist(scale(df2), method = "euclidean")
hc <- hclust(dd, method = "ward.D2")
# and then we can plot the result as a small network
plot(as.phylo(hc), type = "unrooted")
#making the subjective dataframe containing the amount of cluster per participant
folder_path <- "C:/Users/runet/OneDrive/Skrivebord/CogCom/logfiles cleaned"
# List all files in the folder
file_names <- list.files(folder_path, full.names = TRUE)
# Extract filenames without extensions
file_names_without_extension <- tools::file_path_sans_ext(basename(file_names))
# Specify the common part to remove
removal <- "logfile_"
# Remove the common part from all filenames
file_names_for_id <- sub(paste0("^", removal), "", file_names_without_extension)
#this is the dataframe combining id and amount of clusters
cluster <-  c(5, 5, 6, 8, 6, 6, 7, 6, 5, 5, 5, 8, 6, 6, 5, 7, 5, 5)
cluster_size <- c(4.2, 3, 3, 4.25, 3.5, 4, 3.1, 2, 5, 2.2, 2.2, 3.6, 2.6, 5.6, 3, 3.85, 2.4, 5.2)
clusters <- tibble(id = file_names_for_id, clusters = cluster, cluster_switch = (cluster - 1), cluster_size = cluster_size)
#combining into new dataframe
data1 <- full_join(data, clusters)
#anova for whether group predicts the amount of clusters. It does not lol
aggregated_data_clusters <- aggregate(clusters ~ id + group + quality, data = data1, mean)
#aov anova
anovaman <- aov(clusters ~ group, data = aggregated_data_clusters)
#lm anova followup
anovaman_lm <- lm(clusters ~ group, data = aggregated_data_clusters)
#no significant results for either :(
summary(anovaman_lm)
summary(anovaman)
#anova for whether group predicts mean semantic distance
aggregated_data_sem <- aggregate(sem_dist ~ id + group + quality, data = data1, mean)
anovagal <- aov(V1 ~ group, data = aggregated_data_sem)
anovagal_lm <- lm(V1 ~ group, data = aggregated_data_sem)
summary(anovagal_lm)
summary(anovagal)
#counting unique words within participants
unique_words <- tally(group_by(data1, id))
data1 <- full_join(data1, unique_words)
data1 <- data1 %>%
mutate(unique_words = n) %>%
select(-n)
data1$unique_words <- as.numeric(data1$unique_words)
#anova on whether or not unique words can be predicted as a function of group
uniqueman <- aov(unique_words ~ group, data = data1)
#anova shows a significant difference witthin at least one of the groups
summary(uniqueman)
#a turkey test reveals a significant difference between the sex and the control group (P < 0.05)
uniqueturkey <- TukeyHSD(uniqueman)
uniqueturkey
print(c(summary(uniqueman), uniqueturkey))
#making the dataframe "forplot"
forplot <- data1 %>%
select(id, unique_words, group, quality)
#linear model for the same effects
lmwordman <- lm(unique_words ~ group, data = forplot)
summary(lmwordman)
plot(lmwordman)
forhist <- forplot %>%
mutate(unique_words = log(unique_words))
ggplot(forhist,
aes(x = unique_words))+
geom_histogram()
#making a model exploring whether mean semantic distance can be predicted by group
semmodel <- lm(V1 ~ group, data = aggregated_data_sem)
semanova <- aov(V1 ~ group, data = aggregated_data_sem)
summary(semmodel)
summary(semanova)
print(c(summary(semmodel), summary(semanova)))
topmodel <- lmer(unique_words ~ group * trial_number + (1|id), data = data1, REML = F)
summary(topmodel)
emmeans(topmodel)
